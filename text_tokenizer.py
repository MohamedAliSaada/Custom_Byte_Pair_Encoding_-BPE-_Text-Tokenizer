# -*- coding: utf-8 -*-
"""Text Tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16SR7cybeoC9Qr7wezjxt4V2hjM9iiQlb
"""

#!pip install --upgrade datasets

"""# Unicode (UTF-8 bits)

UTF-8 is a variable-length encoding that uses 8-bit units (bytes). Each character can take 1 to 4 bytes depending on its Unicode code point. For example, English characters like "E" need only 1 byte, while Arabic characters like "م" need 2 bytes. All bytes are 8 bits long, and no character uses less than one byte or more than four.
"""

# code point (ord()).
print (ord('E'))  # get 69   need 8bits.
print (ord('م'))  # get 1605 need 16bits.

# show how many bytes we need. ().encode(utf-8)

text1 = 'E'
text2 = 'م'

print(text1.encode('utf-8'))  #b'E' mean one byte.
print(text2.encode('utf-8'))  #b'\xd9\x85' mean two bytes.

"""# Very imporatnt note

utf-8 , give only list of range 0-255 numbers . for one byte this in straight forward , but for two bytes this may not be straight forward . like "م" give you [217, 133] . so utf-8 vocab have 0-255 vocab size at begining .for this note , we use byte pair encoding (BPE) .

# BPE
The BPE merge rules are learned from all the data, so frequent Arabic patterns are merged, just like English ones.
as It learns frequent byte sequences across the whole training data.
It may learn that [216, 167][216, 168] (ا ب) is common in Arabic.
It may also learn [108, 111, 118, 101] ("love") in English.

"""

print(list('محمد'.encode('utf-8')))
print(list('mohamed'.encode('utf-8')))

"""# first we need some data set to train ower tokenizer .

lets use "jw300 en-ar" from hugging face , but use sub of it .
"""

from datasets import load_dataset
import pandas as pd
dataset = load_dataset("sentence-transformers/parallel-sentences-jw300", "en-ar", revision='main')

#now let's take sub set of this data and train ower tokenizer.
data = dataset['train'][5:70]
df = pd.DataFrame(data)

#now let's make ower text for training .
text_ = list(df['english']) + list(df['non_english'])
text = '\n'.join(text_)        #have len 6193 code point

"""# second let's build ower BPE needed functions and loop to get ower new vocab"""

#now let's make ower text to tokens (the 255 basic one).
tokens = list(text.encode('utf-8'))  #have len of 8690 token , as we have arabic - englihs mix (revise : very important note.)

#let's define the function that find the most common pair .

def get_stats(ids):
  counts={}
  for pair in zip(ids,ids[1:]):
    if pair in counts:
      counts[pair] +=1
    else:
      counts[pair] =1
  return sorted(counts.items(),reverse=True,key=lambda item : item[1])[0][0]
# i make function return most common pair only .

#I want now to map , most freq pair to new id , start from 256 , as we have alrady 255 vocab with us.
def merge(ids,m_frequ_pair,new_id):
  new_ids=[]
  i=0
  while i < len(ids):
    if   (i<len(ids)-1)    and ((ids[i],ids[i+1]) == m_frequ_pair) :
      new_ids.append(new_id)
      i +=2
    else :
      new_ids.append(ids[i])
      i +=1
  return new_ids


final_vocab_size =500
num_merges = final_vocab_size-256
ids = tokens

#now loop to get ower tokenizer.
merges={}  #(int,int) to int
for i in range(num_merges):
  if len(ids) <2 :
    break  #no thing to merge
  m_frequ_pair=get_stats(ids)
  new_id=256+i
  ids = merge(ids,m_frequ_pair,new_id)
  merges[m_frequ_pair] = new_id

"""# Now make ower ENCODER and DECODER"""

def encode(text, merges):
    """
    Encode input text string into token ids using merges.

    Args:
      text (str): Input string to encode.
      merges (dict): Dict mapping pairs (int,int) → new_id (int).

    Returns:
      List[int]: Encoded token ids.
    """
    if not isinstance(text, str):
        raise TypeError("Input to encode must be a string.")

    ids = list(text.encode('utf-8'))  # text → list of UTF-8 bytes

    applied_merges = True
    while applied_merges:
        applied_merges = False
        i = 0
        new_ids = []
        while i < len(ids):
            if i < len(ids) - 1 and (ids[i], ids[i + 1]) in merges:
                new_ids.append(merges[(ids[i], ids[i + 1])])
                i += 2
                applied_merges = True
            else:
                new_ids.append(ids[i])
                i += 1
        ids = new_ids

    return ids


def decode(ids, reverse_merges):
    """
    Decode a single token id or a list of token ids back to text string.

    Args:
      ids (int or List[int] or Tuple[int]): Token id(s) to decode.
      reverse_merges (dict): Dict mapping new_id → pair (int,int).

    Returns:
      str: Decoded UTF-8 string.
    """
    # Normalize input to list
    if isinstance(ids, int):
        ids = [ids]
    elif not isinstance(ids, (list, tuple)):
        raise TypeError("Input to decode must be int or list/tuple of ints.")

    # Undo merges recursively
    while True:
        updated = False
        new_ids = []
        for token in ids:
            if token in reverse_merges:
                a, b = reverse_merges[token]
                new_ids.extend([a, b])
                updated = True
            else:
                new_ids.append(token)
        ids = new_ids
        if not updated:
            break

    byte_seq = bytes(ids)
    return byte_seq.decode('utf-8', errors='replace')

#test ower tokenizer

text = "man"  # Arabic example

# Encode
token_ids = encode(text, merges)
print("Token IDs:", token_ids)

reverse_merges = {v: k for k, v in merges.items()}

# Decode
reconstructed_text = decode(token_ids, reverse_merges)
print("Decoded Text:", reconstructed_text)



"""# get output file of merges used after that to decode and encode"""

with open('merges500ids.txt','w') as f:
  for (x,y),id in merges.items():
    f.write(f'{x},{y},{id}\n')

